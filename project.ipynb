{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Da fare\n",
    "1. Car model classification:\n",
    "   - entire car images vs **car parts**\n",
    "   - **specific viewpoint** vs all viewpoints\n",
    "2. **Car make classification**\n",
    "3. **Attribute Prediction:**\n",
    "   - Sum of square loss for continuous attributes\n",
    "   - Logistic loss for for discrete attributes\n",
    "   - Trained of the first set and tested on the second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import Module, Sequential, Conv2d, BatchNorm2d\n",
    "from torch.nn import SiLU as ReLU\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "from torchvision.transforms import Compose, ToTensor, RandomAffine, RandomHorizontalFlip, RandomVerticalFlip, ColorJitter, Resize\n",
    "import torch\n",
    "\n",
    "import utils\n",
    "import dataset \n",
    "import ResNet18_blocks\n",
    "import ResNet50_blocks\n",
    "import training_functions\n",
    "from dataset import indexing_labels, check_unbalance_dataset, ImageDataset\n",
    "from utils import plot_losses, exponential_moving_average\n",
    "from ResNet18_blocks import BasicBlock, ResNet18\n",
    "from ResNet50_blocks import MainPath, IdentityBlock, ConvolutionalBlock, ResNet50\n",
    "from training_functions import network_training, transforms_train, transforms, FocalLoss, evaluate_network\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(utils)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(training_functions)\n",
    "importlib.reload(ResNet18_blocks)\n",
    "importlib.reload(ResNet50_blocks);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDgcLN462u-1"
   },
   "source": [
    "# Neural Network and Deep Learning Project\n",
    "\n",
    "*Alberto Salvador, Volpi Gaia*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "_V2RXqilaw49",
    "outputId": "e1483d51-65dd-470d-b319-84f1e7225a9b"
   },
   "outputs": [],
   "source": [
    "volume_dir = \"/mnt/shared_volume/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "0HwsecSe8bDT",
    "outputId": "eeef1e8b-ac1c-4ee6-9b82-6cf714acbc94"
   },
   "outputs": [],
   "source": [
    "! ls /mnt/shared_volume/data/train_test_split/classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vxGKKCY14DF"
   },
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_index = indexing_labels(volume_dir+\"data/train_test_split/classification/train.txt\")\n",
    "print(\"Nr of models in the train dataset: \", len(label_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAtRu4c1jA33"
   },
   "source": [
    "viewpoint:\n",
    "- -1 - uncertain\n",
    "- 1 - front\n",
    "- 2 - rear\n",
    "- 3 - side\n",
    "- 4 - front-side\n",
    "- 5 - rear-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51W3vn_cB3iC"
   },
   "outputs": [],
   "source": [
    "viewpoint = None\n",
    "train_dataset =  ImageDataset(volume_dir +\"data\", volume_dir + \"data/train_test_split/classification/train.txt\", label_to_index, transforms_train, viewpoint)\n",
    "test_dataset =  ImageDataset(volume_dir +\"data\", volume_dir +\"data/train_test_split/classification/test_updated.txt\", label_to_index, transforms, viewpoint)\n",
    "valid_dataset =  ImageDataset(volume_dir +\"data\", volume_dir +\"data/train_test_split/classification/valid.txt\", label_to_index, transforms, viewpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "NjVO575OCR4r",
    "outputId": "004cef8f-c9b5-48b3-96cd-9d8d1bf73531"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 4, figsize=(6,6))\n",
    "for i in range(16):\n",
    "    axs[i//4][i%4].imshow(train_dataset[i][0].permute(1, 2, 0).numpy())\n",
    "    axs[i//4][i%4].set_xticks([])\n",
    "    axs[i//4][i%4].set_yticks([])\n",
    "    axs[i//4][i%4].set_title(f\"{train_dataset[i][1]}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bjHs8vuG6eq",
    "outputId": "dff480a3-6c13-4a7b-b243-9a07fbe31733"
   },
   "outputs": [],
   "source": [
    "print(f'train dataset: {len(train_dataset)} \\ntest dataset: {len(test_dataset)} \\nvalid dataset: {len(valid_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SBtLucZ-xgT"
   },
   "outputs": [],
   "source": [
    "# Here we use the Dataloader function from pytorch to opportunely split the dataset in batches and shuffling data\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=os.cpu_count())\n",
    "valid_dataloader = DataLoader(valid_dataset, os.cpu_count()*2, shuffle=False, num_workers=os.cpu_count())\n",
    "test_dataloader = DataLoader(test_dataset, os.cpu_count()*2, shuffle=False, num_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "check_unbalance_dataset(valid_dataloader, n_indices=3000, title='validation set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "check_unbalance_dataset(train_dataloader, n_indices=3000, title='training set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "check_unbalance_dataset(valid_dataloader, n_indices=3000, title='valid set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9H22d0w008G",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. ResNet50() architecture\n",
    "\n",
    "### 2.1 Identity block\n",
    "\n",
    "The identity block corresponds to the case where the input activation ($a^{[l]}$) has the same dimension as the output activation ($a^{[l+2]}$).\n",
    "We implement the case in which the skip connection \"skips over\" 3 hidden layers.\n",
    "\n",
    "The upper path is the \"shortcut path\". The lower path is the \"main path\" with convolutional layers and ReLu activation functions. To speed up training we also add BatchNorm layers.  \n",
    "\n",
    "Here are the individual steps:\n",
    "\n",
    "**First component** of main path:\n",
    "- CONV2D with $F_1$ filters of shape (1, 1), stride of (1, 1), padding \"valid\".\n",
    "- BatchNorm, normalizing the 'channels' axis.\n",
    "- ReLU activation function.\n",
    "\n",
    "**Second component** of main path:\n",
    "- CONV2D with $F_2$ filters of shape $(f, f)$, stride of (1, 1), padding \"same\".\n",
    "- BatchNorm, normalizing the 'channels' axis.\n",
    "- ReLU activation function.\n",
    "\n",
    "**Third component** of main path:\n",
    "- CONV2D with $F_3$ filters of shape (1, 1), stride of (1, 1), padding \"valid\".\n",
    "- BatchNorm, normalizing the 'channels' axis.\n",
    "\n",
    "**Final step**:\n",
    "- Add together the `X_input` and the output from the 3rd layer `X` (shortcut).\n",
    "- ReLU activation function.\n",
    "![Alt text](identity_block.png)\n",
    "\n",
    "NOTA: \n",
    "- padding *valid*: p=0 (default). No padding → output size is smaller than input size\n",
    "- padding *same*: p=kernel_size // 2. → output size equals input size (when stride=1)\n",
    "\n",
    "NOTA:\n",
    "- Per la cross entropy non serve le labels siano one-hot encoded ma solo indexed, e all'ouput della network non deve essere stata applicata non-linear function (softmax)\n",
    "\n",
    "### 2.2 Convolutional block\n",
    "\n",
    "You can use this type of block when the input and output dimensions don't match up. The difference with the identity block is that there is a CONV2D layer in the shortcut path.\n",
    "\n",
    "* The CONV2D layer in the shortcut path is used to resize the input $x$ to a different dimension, so that the dimensions match up in the final addition needed to add the shortcut value back to the main path.  \n",
    "* For example, to reduce the activation dimensions's height and width by a factor of 2, you can use a 1x1 convolution with a stride of 2.\n",
    "* The CONV2D layer on the shortcut path does not use any non-linear activation function. Its main role is to just apply a (learned) linear function that reduces the dimension of the input, so that the dimensions match up for the later addition step.\n",
    "\n",
    "The details of the convolutional block are as follows:\n",
    "\n",
    "**First component** of main path:\n",
    "- CONV2D with $F_1$ filters of shape (1, 1), stride of (s, s), padding \"valid\".\n",
    "- BatchNorm.\n",
    "- ReLU activation function.\n",
    "\n",
    "**Second component** of main path:\n",
    "- CONV2D with $F_2$ filters of shape (f, f), stride of (1, 1), padding \"same\".\n",
    "- BatchNorm.\n",
    "- ReLU activation function.\n",
    "\n",
    "**Third component** of main path:\n",
    "- CONV2D with $F_3$ filters of shape (1, 1), stride of (1,1), padding \"valid\".\n",
    "- BatchNorm.\n",
    "Note that there is no ReLU activation function in this component.\n",
    "\n",
    "**Shortcut path**:\n",
    "- CONV2D with $F_3$ filters of shape (1, 1), stride of (s, s), padding \"valid\".\n",
    "- BatchNorm.\n",
    "\n",
    "**Final step**:\n",
    "- Add together the shortcut and the main path values.\n",
    "- ReLU activation function.\n",
    "\n",
    "![Alt text](convolutional_block.png)\n",
    "\n",
    "### 2.3 Model\n",
    "\n",
    "The details of this ResNet-50 model are:\n",
    "- Stage 1:\n",
    "    - 2D Convolution with 64 filters of shape (7, 7) and stride of (2, 2).\n",
    "    - BatchNorm, applied to the 'channels' axis of the input.\n",
    "    - MaxPooling with (3, 3) window and (2, 2) stride.\n",
    "- Stage 2:\n",
    "    - Convolutional block with three sets of filters of size [64, 64, 256], `f = 3` and `s = 1`.\n",
    "    - 2 identity blocks with three sets of filters of size [64,64,256], `f = 3`.\n",
    "- Stage 3:\n",
    "    - Convolutional block with three sets of filters of size [128, 128, 512], `f = 3` and `s = 2`.\n",
    "    - 3 identity blocks with three sets of filters of size [128, 128, 512], `f = 3`.\n",
    "- Stage 4:\n",
    "    - Convolutional block with three sets of filters of size [256, 256, 1024], `f = 3` and `s = 2`.\n",
    "    - 5 identity blocks with three sets of filters of size [256, 256, 1024], `f = 3`.\n",
    "- Stage 5:\n",
    "    - Convolutional block with three sets of filters of size [512, 512, 2048], `f = 3` and `s = 2`.\n",
    "    - 2 identity blocks with three sets of filters of size [512, 512, 2048], `f = 3`.\n",
    "- 2D Average Pooling with window of shape (2, 2).\n",
    "- Flatten layer.\n",
    "- Fully Connected (Dense) layer reduces its input to one single neuron at output using sigmoid activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Resnet18()  architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_array = []\n",
    "for batch in train_dataloader:\n",
    "    _, labels = batch\n",
    "    labels_array.append(labels)\n",
    "labels_array = torch.cat(labels_array).numpy().astype(int)\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels_array), y=labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, train_loss_log, val_loss_log  = network_training(class_weights, train_dataloader, valid_dataloader, model=ResNet50(), opt=Adam, loss_fn=FocalLoss(), epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(train_loss_log, val_loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "model = ResNet50()\n",
    "model.load_state_dict(torch.load(volume_dir+\"model.pt\"))\n",
    "k_list = [1,2,3,4,5]\n",
    "topk_accuracy_train = evaluate_network(train_dataloader, model, k_list, \"Training Dataset\")\n",
    "topk_accuracy_valid = evaluate_network(valid_dataloader, model, k_list, \"Validation Dataset\")\n",
    "topk_accuracy_test = evaluate_network(test_dataloader, model, k_list, \"Test Dataset\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
